{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "# Converting a TensorFlow* Model in Linux*\n",
    "\n",
    "## Pre-Requisites:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Install Tensorflow 1.12 version\n",
    "\n",
    "Run the following cells to install tensorflow 1.12 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install --user tensorflow==1.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: you may need to restart the kernel to use updated packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new temporary directory by running the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! export TMPDIR=/tmp/$USER; mkdir -p $TMPDIR; python -c \"import tempfile; print(tempfile.gettempdir())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name squeezenet1.1 -o models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the Inference engine samples in Devcloud\n",
    "\n",
    "**Note** : It might take sometime if the samples are not already built on your account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /opt/intel/openvino/inference_engine/samples/build_samples.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Models from TensorFlow* -slim library\n",
    "\n",
    "There are a number of pre-trained public models in the TensorFlow*-slim repository. The models are distributed as Python scripts and checkpoint files. First of all download repository with models. Note: This is done in home directory(~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd && git clone https://github.com/tensorflow/models/ && cd models/research/slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Export Inference Graph and download checkpoint file\n",
    "\n",
    "Export inference graph for one of the available models using the following command (Inception V1 in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -W ignore  $HOME/models/research/slim/export_inference_graph.py --alsologtostderr --model_name=inception_v1 --output_file=/tmp/$USER/inception_v1_inf_graph.pb  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "The script creates inference graph file with name “inception_v1_inf_graph.pb” in the /tmp direcory.\n",
    "\n",
    "### Download archive with checkpoint file (Inception V1 in this example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "export CHECKPOINT_DIR=/tmp/$USER/checkpoints\n",
    "\n",
    "mkdir $CHECKPOINT_DIR\n",
    "\n",
    "wget http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\n",
    "\n",
    "tar -xvf inception_v1_2016_08_28.tar.gz\n",
    "\n",
    "mv inception_v1.ckpt $CHECKPOINT_DIR\n",
    "\n",
    "rm inception_v1_2016_08_28.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "sh run.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### Freeze the model\n",
    "\n",
    "The last step is to freeze the graph. To do this you need to know the output node of the model you are planning to freeze. This information is found by running the summarize_graph.\n",
    "\n",
    "### Summarize Graph\n",
    "\n",
    "Go to ~/models/research/slim/ directory and run summarize_graph.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! cd $HOME/models/research/slim/ && python3 -W ignore  /opt/intel/openvino/deployment_tools/model_optimizer/mo/utils/summarize_graph.py --input_model=/tmp/$USER/inception_v1_inf_graph.pb \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "The output layer/node name should be in the last line of text on the console and should look like:\n",
    "\n",
    "1 input(s) detected: Name: input, type: float32, shape: (-1,224,224,3) 1 output(s) detected: InceptionV1/Logits/Predictions/Reshape_1\n",
    "\n",
    "### Freeze the graph for Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "The script generates inception_v1_frozen.pb file with the frozen model in the directory you are currently in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -W ignore /usr/local/lib/python3.6/dist-packages/tensorflow/python/tools/freeze_graph.py --input_graph /tmp/$USER/inception_v1_inf_graph.pb --input_binary --input_checkpoint /tmp/$USER/checkpoints/inception_v1.ckpt --output_node_names InceptionV1/Logits/Predictions/Reshape_1 --output_graph inception_v1_frozen.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "You might get warning message \"Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\" while executing above command, ignore that.\n",
    "## Convert Frozen Tensorflow* model to IR using Model Optimizer\n",
    "\n",
    "Assuming you are in the ~/models/research/slim/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -W ignore  /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_model inception_v1_frozen.pb --input_shape [1,224,224,3] --mean_values [1024,1024,1024] --scale_values [128,128,128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "This should produce “inception_v1_frozen.xml” and “inception_v1_frozen.bin” file. The xml file contains the topology information of the model and the bin file contains the model’s weights and biases. These two files are expected when using the inference engine and so make note of the path.\n",
    "\n",
    "## Run Classification Sample\n",
    "\n",
    "The classification sample will showcase the Intel® Distribution of OpenVINO™ toolkit inference engine using TensorFlow model Inception_v1_frozen IR files (.xml & .bin) and an input image of a car to classify. The classification collateral is defined as the input image car_1.bmp, the Inception_v1_frozen IR files (.xml & .bin), and the labels file inception_v1_frozen.labels. Create a new directory that will hold the classification sample app and all needed components to run the classification sample Note: The following steps should be followed and are assuming you are following the preceding steps. You should be in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /opt/intel/openvino/deployment_tools/demo/squeezenet1.1.labels ./inception_v1_frozen.labels >/dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "#### Run Application\n",
    "\n",
    "Note: To see all the flags that the sample takes as input, run the below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ~/inference_engine_samples_build/intel64/Release/classification_sample -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the [car_1.bmp](car_1.bmp) image below to capture attributes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ~/inference_engine_samples_build/intel64/Release/classification_sample_async -i car_1.bmp  -m inception_v1_frozen.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Expected Output:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    "<img src=\"classification_output.png\"> \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
