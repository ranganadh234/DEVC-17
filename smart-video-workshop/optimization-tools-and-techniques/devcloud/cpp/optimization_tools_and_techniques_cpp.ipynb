{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Computer Vision Applications\n",
    "\n",
    "This tutorial shows some techniques to get better performance for computer vision applications with the Intel® Distribution of OpenVINO™ toolkit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup the environment variables,download model files and import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name mobilenet-ssd -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name ssd300 -o models\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name ssd512 -o models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p models/object_detection/SSD512/{FP16,FP32} \n",
    "!mkdir -p models/object_detection/SSD300/{FP16,FP32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Optimizer on the models to get IR files\n",
    "\n",
    "First, we will create the required directories, then run the model Optimizer to get the IR files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/mobilenet-ssd/mobilenet-ssd.caffemodel -o models/mobilenet-ssd/FP32/ --scale 256 --mean_values [127,127,127]\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/mobilenet-ssd/mobilenet-ssd.caffemodel -o models/mobilenet-ssd/FP16/ --scale 256 --mean_values [127,127,127] --data_type FP16\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/ssd300/models/VGGNet/VOC0712Plus/SSD_300x300_ft/VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.caffemodel --input_proto models/public/ssd300/models/VGGNet/VOC0712Plus/SSD_300x300_ft/deploy.prototxt  -o models/SSD300/FP32/\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/ssd300/models/VGGNet/VOC0712Plus/SSD_300x300_ft/VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.caffemodel --input_proto models/public/ssd300/models/VGGNet/VOC0712Plus/SSD_300x300_ft/deploy.prototxt -o models/SSD300/FP16/ --data_type FP16\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/ssd512/models/VGGNet/VOC0712Plus/SSD_512x512/VGG_VOC0712Plus_SSD_512x512_iter_240000.caffemodel --input_proto models/public/ssd512/models/VGGNet/VOC0712Plus/SSD_512x512/deploy.prototxt -o models/SSD512/FP32/\n",
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model models/public/ssd512/models/VGGNet/VOC0712Plus/SSD_512x512/VGG_VOC0712Plus_SSD_512x512_iter_240000.caffemodel --input_proto models/public/ssd512/models/VGGNet/VOC0712Plus/SSD_512x512/deploy.prototxt -o models/SSD512/FP16/ --data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pick the right model based on application and hardware\n",
    "\n",
    "Use/train a model with the right performance/accuracy tradeoffs. Performance differences between models can be bigger than any optimization you can do at the inference app level. Run various SSD models from the model_downloader in the car detection example which we used in the initial tutorial and observe the performance. We will run these tests on different hardware accelerators to determine how application performance depends on models as well as hardware.\n",
    "\n",
    "In the previous step we have all the models convered and ready by model Optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set environmental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('Cars video', ['cars_1900.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the code\n",
    "\n",
    "The code in this demo is separated into two parts.\n",
    "First part is responsible for reading the input stream and running the object detection inference workload on the stream. \n",
    "This part outputs Region Of Interest (ROI), in terms of coordinates, for each frame.\n",
    "The source code for this part can be found in [main.cpp](./main.cpp), and the executable will be named \"tutorial1\".\n",
    "Output ROI will be written into a text file, \"ROIs.txt\".\n",
    "\n",
    "The second part reads the ROIs.txt file, and overlays boxes on each frame of the stream based on the coordinates.\n",
    "Then the output video is written into a file. \n",
    "The source code for this step is in [ROI_writer.cpp](./ROI_writer.cpp).\n",
    "\n",
    "We have provided a Makefile for compiling the examples. Run the following cell to compile the application.\n",
    "(tip: use **crtl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commandline flags\n",
    "\n",
    "The two executables, tutorial1 and ROIwriter, take a number of commandline arguments.\n",
    "\n",
    "Run the following cells to see the list of the available arguments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! ./tutorial1 -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./ROI_writer -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Job Script \n",
    "\n",
    "We will run the workload on several DevCloud's edge compute nodes. We will send work to the edge compute nodes by submitting jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file will be executed directly on the edge compute node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "MODEL=$3\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "#  The output directory is the first argument of the bash script\n",
    "mkdir -p $OUTPUT_FILE\n",
    "ROIFILE=$OUTPUT_FILE/ROIs.txt\n",
    "OVIDEO=$OUTPUT_FILE/output.mp4\n",
    "\n",
    "if [ \"$MODEL\" = \"FP32\" ]; then\n",
    "    config_file=\"conf_fp32.txt\"\n",
    "else\n",
    "    config_file=\"conf_fp16.txt\"\n",
    "fi\n",
    "\n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "./tutorial1 -i /data/reference-sample-data/object-detection-python/cars_1900.mp4 \\\n",
    "            -m $MODEL \\\n",
    "            -d $DEVICE \\\n",
    "            -o $OUTPUT_FILE\\\n",
    "            -fr 3000\n",
    "\n",
    "# Converting the text output to a video\n",
    "./ROI_writer -i /data/reference-sample-data/object-detection-python/cars_1900.mp4 \\\n",
    "             -o $OUTPUT_FILE \\\n",
    "             -ROIfile $ROIFILE \\\n",
    "             -l pascal_voc_classes.txt \\\n",
    "             -r 2.0 # output in half res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the object detection example with different models on different devices.\n",
    "\n",
    "For simplicity of the code and in order to put more focus on the performance number, video rendering with rectangle boxes for detected objects has been separated from object detection example(tutorial1.py). The inference difference in different scenarios can be seen in the progress bar after running the sample. \n",
    "\n",
    "\n",
    "### a) CPU\n",
    "\n",
    "#### - Inferencing using **mobilenet-ssd** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/core/mobilenet CPU models/mobilenet-ssd/FP32/mobilenet-ssd.xml\" -N obj_det_cpu\n",
    "print(job_id_core[0])\n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "   progressIndicator('results/core/mobilenet', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "   progressIndicator('results/core/mobilenet', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using **ssd300** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:i5-6500te -F \"results/Core/ssd300 CPU models/SSD300/FP32/VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.xml\" -N obj_det_cpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core/ssd300', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core/ssd300', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using **ssd512** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:i5-6500te -F \"results/Core/ssd512 CPU models/SSD512/FP32/VGG_VOC0712Plus_SSD_512x512_iter_240000.xml\" -N obj_det_cpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core/ssd512', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core/ssd512', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) GPU\n",
    "\n",
    "#### - Inferencing using **mobilenet-ssd** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/Core/mobilenet GPU models/mobilenet-ssd/FP32/mobilenet-ssd.xml\" -N obj_det_gpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core/mobilenet', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core/mobilenet', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using model: ssd300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/Core/ssd300 GPU models/SSD300/FP32/VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.xml\" -N obj_det_gpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core/ssd300', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core/ssd300', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using model: ssd512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/Core/ssd512 GPU models/SSD512/FP32/VGG_VOC0712Plus_SSD_512x512_iter_240000.xml\" -N obj_det_gpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/Core/ssd512', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/Core/ssd512', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Intel® Movidius™ Neural Compute Stick\n",
    "\n",
    "#### - Inferencing using **mobilenet-ssd** model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"results/ncs/mobilenet MYRIAD models/mobilenet-ssd/FP32/mobilenet-ssd.xml\" -N obj_det_ncs2\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/ncs/mobilenet', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/ncs/mobilenet', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using model: ssd300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"results/ncs/ssd300 MYRIAD models/SSD300/FP32/VGG_VOC0712Plus_SSD_300x300_ft_iter_160000.xml\" -N obj_det_ncs2\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/ncs/ssd300', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/ncs/ssd300', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### - Inferencing using model: ssd512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc004nc2:intel-ncs2 -F \"results/ncs/ssd512 MYRIAD models/SSD512/FP32/VGG_VOC0712Plus_SSD_512x512_iter_240000.xml\" -N obj_det_ncs2\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/ncs/ssd512', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/ncs/ssd512', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use the right data type for your target hardware and accuracy needs\n",
    "\n",
    "In this section, we will consider an example running on a GPU. FP16 operations are better optimized than FP32 on GPUs. We will run the object detection example with SSD models with data types FP16 and FP32 and observe the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/GPU/mobilenet GPU models/mobilenet-ssd/FP32/mobilenet-ssd.xml\" -N obj_det_gpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/GPU/mobilenet', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/GPU/mobilenet', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:idc001skl:intel-hd-530 -F \"results/GPU/mobilenet GPU models/mobilenet-ssd/FP16/mobilenet-ssd.xml\" -N obj_det_gpu\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/GPU/mobilenet', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/GPU/mobilenet', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that we got better performance with FP16 models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
