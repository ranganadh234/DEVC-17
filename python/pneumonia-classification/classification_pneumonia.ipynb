{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia detection in chest X-ray images\n",
    "\n",
    "Pneumonia is an inflammatory condition of the lung affecting primarily the small air sacs known as alveoli.\n",
    "Typically symptoms include some combination of cough, chest pain, fever, and trouble breathing [1].\n",
    "Pneumonia affects approximately 450 million people globally (7% of the population) and results in about 4 million deaths per year [2]. To diagnose this disease, chest X-ray images remain the best diagnosis tool.\n",
    "\n",
    "![](example-pneumonia.jpeg) *Chest X-ray image of a patient with Pneumonia*\n",
    "\n",
    "In this tutorial, we use a model trained to classify patients with pneumonia over healthy cases based on their chest X-ray images. The topology used is the DenseNet 121, this architecture has shown to be very efficient at this problem, it was the first work to claim classification rate better than practicing radiologists. The dataset used for training is from the \"Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification\" [3] with a CC BY 4.0 license.\n",
    "The trained model is provided as a frozen Tensorflow model (.pb).\n",
    "\n",
    "\n",
    "**In this tutorial, we perform inference on this model using the OpenVINO(TM) toolkit on the validation dataset on multiple hardware targets such as different Intel CPUS, Neural Compute Stick, the Intel® Arria® 10 FPGA, etc...\n",
    "We will also visualize what the network has learned using the technique of Class Activation Maps.**\n",
    "\n",
    "[1] Ashby B, Turkington C (2007). The encyclopedia of infectious diseases (3rd ed.). New York: Facts on File. p. 242. ISBN 978-0-8160-6397-0. \n",
    "\n",
    "[2] Ruuskanen O, Lahti E, Jennings LC, Murdoch DR (April 2011). \"Viral pneumonia\". Lancet. 377 (9773): 1264–75. \n",
    "\n",
    "[3] Labeled Optical Coherence Tomography (OCT) and Chest X-Ray Images for Classification, http://dx.doi.org/10.17632/rscbjbr9sj.2#file-41d542e7-7f91-47f6-9ff2-dd8e5a5a7861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to convert the model to OpenVINO Intermediate Representation through the Model Optimizer. The MO script is *mo_tf.py*, which takes as input:  \n",
    "-m: path to the input model (.pb)  \n",
    "\n",
    "--input_shape: dimension of the input tensor (optional for most of the models)\n",
    "\n",
    "--data_type: data type of the IR model (FP32 for CPU, FP16 for GPU, Movidius and FPGA)\n",
    "\n",
    "-o : path where to save the IR\n",
    "\n",
    "--mean_values : pre-processing mean values with which the model was trained with \n",
    "\n",
    "--scale_values : pre-processing scale values with which the model was trained with \n",
    "\n",
    "The second argument is not necessary for  models with a defined input shape. However, Tensorflow models often have dynamic input shapes (often the batch will be set as -1), which is not supported by OpenVINO.\n",
    "The mean and scale values are also optional, it however simplifies the inference later as it includes automatically the pre-processing (scaling and averaging) inside the model directly. For this model, the model was trained with images following the ImageNet pre-processing. \n",
    "\n",
    "First, we convert the model to a FP32 IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP32 -o models/FP32 --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert to FP16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py -m model.pb --input_shape=[1,224,224,3] --data_type FP16 -o models/FP16/ --mean_values [123.75,116.28,103.58] --scale_values [58.395,57.12,57.375]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the result Intermediate Representation (.xml). You can observe the different layers with informations such as name, operation type, precision, input shape and output shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "bs = BeautifulSoup(open('models/FP32/model.xml'), 'xml')\n",
    "print(bs.prettify()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are able to perform inference with OpenVINO. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Inference with OpenVINO on the server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will perform inference on the server (Xeon SkyLake) in order to showcase the Python API of the OpenVINO inference engine. \n",
    "Then, we will showcase how to use our API to compute the Class Activation Maps. \n",
    "In Section 3, we will perform inference on other hardware target devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import several useful Python packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from time import time\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib import cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set several variables such as path to the IR model, the target device and path to the images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xml = \"models/FP32/model.xml\"\n",
    "device=\"CPU\"\n",
    "files_pneumonia=glob.glob(os.getcwd()+'/validation_images/PNEUMONIA/*.jpeg')\n",
    "files_healthy=glob.glob(os.getcwd()+'/validation_images/NORMAL/*.jpeg')\n",
    "colormap='viridis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define an image reading function that uses OpenCV for image loading, resize it to the network input size, and reshape the image numpy array into CHW (Channels, Height, Width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_img, img_to_array, resize_image\n",
    "\n",
    "def read_image(path):\n",
    "    image_original = load_img(path, color_mode=\"rgb\")\n",
    "    img= resize_image(image_original, target_size=(224, 224))\n",
    "    x = img_to_array(img, data_format='channels_first')\n",
    "    return [x,image_original]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to perform the classification of the pneumonia cases over the healthy patients.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bin = os.path.splitext(model_xml)[0] + \".bin\"\n",
    "\n",
    "# Plugin initialization for specified device and load extensions library if specified\n",
    "plugin = IEPlugin(device=device)\n",
    "\n",
    "# Read IR\n",
    "net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "assert len(net.inputs.keys()) == 1, \"Sample supports only single input topologies\"\n",
    "\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = next(iter(net.outputs))\n",
    "\n",
    "net.batch_size = 1\n",
    "\n",
    "exec_net = plugin.load(network=net)\n",
    "\n",
    "n,c,h,w=net.inputs[input_blob].shape\n",
    "case=0\n",
    "\n",
    "print(\"Pneumomina cases\")\n",
    "for file in sorted(files_pneumonia):\n",
    "    [image1,image]= read_image(file)\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0][0]\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: pneumonia case), performed in \" + str(infer_time) +\" ms\" )\n",
    "        \n",
    "print(\"Healthy cases\")        \n",
    "for file in sorted(files_healthy):\n",
    "    [image1,image]= read_image(file)\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0][0]\n",
    "    plt.imshow(image)\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: healthy case), performed in \" + str(infer_time) +\" ms\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize what the model has learned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class activation maps (CAM) [1] are a simple technique to visualize the regions that are relevant to a Convolutional Neural Network to identify a specific class in the image. \n",
    "The CAM $M(x,y)$ is calculated from the N feature maps $f_i(x,y)$ from the last convolutional layer. We perform the weighted sum of those feature maps based on the weigths of the fully connected layer $w_i$ , which represents how important are those feaure maps for the classification output. \n",
    "\n",
    "$ M(x,y)=\\sum_{i=0}^N w_i f_i(x,y) $\n",
    "\n",
    "[1] Zhou, Bolei, et al. \"Learning deep features for discriminative localization.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate Class Activation Maps in OpenVINO, we need to access the output feature maps of the last convolution layer and the weights of the fully connected layer. \n",
    "By default, only the output layer is the output (obviously). Therefore, in order to get the feature maps, we need to add the last convolution as an additional output. \n",
    "This is simply done by using the function *add_outputs(\"layer_name\")* on the network before loading it to the plugin. \n",
    "In order to obtain the layers name, we recommend Netron, which allows to visualize graphically the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get the feature maps, inference must be performed first. This is why our function to calculate the Class Action Maps requires the inference output (which includes the feature maps since we added an additional output previously).\n",
    "We access the FC layer weights using the call *net.layers.get(\"name_of_fully_connected_layer\").weights[\"weights\"]*\n",
    "\n",
    "Then, we perform the weighted sum of the weights with the feature maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_class_activation_map_openvino(res, bn, fc, net):\n",
    "    res_bn = res[bn]\n",
    "    weights_fc=net.layers.get(fc).weights[\"weights\"]\n",
    "    conv_outputs=res_bn[0,:,:,:]\n",
    "    cam = np.zeros(dtype=np.float32, shape=(conv_outputs.shape[1:]))\n",
    "    for i, w in enumerate(weights_fc):\n",
    "        cam += w * conv_outputs[i, :, :]\n",
    "    return cam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined the generic function in order to obtain the Class Activation Maps, let's use it now for our example. \n",
    "\n",
    "We define first the name of the last convolutional layer from which we need to extract the feature maps. In our example, it is the layer with ID=365. \n",
    "We also set the name of the Fully-Connected layer, in our case **predictions_1/MatMul**\n",
    "\n",
    "For our tutorial, we parse the xml in order to get the name of the last Convolutional layer but you can also directly provide it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsing the IR in order to find the last convolutional layer \n",
    "from bs4 import BeautifulSoup\n",
    "bs = BeautifulSoup(open('models/FP32/model.xml'), 'xml')\n",
    "bnTag = bs.findAll(attrs={\"id\" : \"365\"})\n",
    "bn = bnTag[0]['name']\n",
    "print(bn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the last convolutional layer as output \n",
    "net.add_outputs(bn)\n",
    "fc=\"predictions_1/MatMul\"\n",
    "\n",
    "# name of the inputs and outputs\n",
    "input_blob = next(iter(net.inputs))\n",
    "out_blob = \"predictions_1/Sigmoid\"\n",
    "\n",
    "#set the batch size\n",
    "net.batch_size = 1\n",
    "\n",
    "# Loading model to the plugin\n",
    "print(\"Loading model to the plugin\")\n",
    "exec_net = plugin.load(network=net)\n",
    "\n",
    "#obtain the shape of the input\n",
    "n,c,h,w=net.inputs[input_blob].shape\n",
    "\n",
    "#iterate over the pneumonia cases\n",
    "for file in sorted(files_pneumonia):\n",
    "    #read the image\n",
    "    [image1,image]= read_image(file)\n",
    "    # Start inference\n",
    "    t0 = time()\n",
    "    res = exec_net.infer(inputs={input_blob: image1})\n",
    "    infer_time=(time() - t0) * 1000\n",
    "    \n",
    "    # Process the classification output\n",
    "    res_pb = res[out_blob]\n",
    "    probs=res_pb[0]\n",
    "    print(\"Probability of having disease= \"+str(probs)+\" (ground truth: pneumonia case), performed in \" + str(infer_time) +\" ms\")\n",
    "    \n",
    "    # Class Activation Map    \n",
    "    t0 = time()\n",
    "    cam=visualize_class_activation_map_openvino(res, bn, fc , net)\n",
    "    cam_time=(time() - t0) * 1000\n",
    "    print(\"Time for CAM: {} ms\".format(cam_time))\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    # Visualize the CAM heatmap\n",
    "    cam /= np.max(cam)\n",
    "    fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(cam, cmap=colormap)\n",
    "    plt.colorbar(fraction=0.046, pad=0.04)\n",
    "    \n",
    "    # Visualize the CAM overlaid over the X-ray image \n",
    "    colormap_val=cm.get_cmap(colormap)  \n",
    "    imss=np.uint8(colormap_val(cam)*255)\n",
    "    im = Image.fromarray(imss)\n",
    "    width, height = image.size\n",
    "    cam1=resize_image(im, (height,width))\n",
    "    heatmap = np.asarray(cam1)\n",
    "    img1 = heatmap [:,:,:3] * 0.3 + image\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    \n",
    "    plt.imshow(np.uint16(img1))\n",
    "    plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inference on the edge\n",
    "\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated a single core. \n",
    "We will run the workload on other edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook. It performs the classification using the script classification_pneumonia.py which is very similar to the Python code that we developed in Section 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile classification_pneumonia_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "OUTPUT_FILE=$1\n",
    "DEVICE=$2\n",
    "FP_MODEL=$3\n",
    "# Traffic detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/opt/altera/aocl-pro-rte/aclrte-linux64/\n",
    "    source /opt/fpga_support_files/setup_env.sh\n",
    "    aocl program acl0 /opt/intel/openvino/bitstreams/a10_vision_design_bitstreams/2019R1_PL1_FP11_MobileNet_Clamp.aocx\n",
    "fi\n",
    "\n",
    "# Running the traffic detection code\n",
    "SAMPLEPATH=${PBS_O_WORKDIR}\n",
    "echo ${1}\n",
    "python3 classification_pneumonia.py  -m models/$FP_MODEL/model.xml  \\\n",
    "                                           -i /validation_images/PNEUMONIA/*.jpeg \\\n",
    "                                           -o $OUTPUT_FILE \\\n",
    "                                           -d $DEVICE\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit ocr_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "The `-F` flag is used to pass in arguments to the job script.\n",
    "The [classification_pneumonia_job.sh](classification_pneumonia_job.sh) takes in 3 arguments:\n",
    "1. the path to the directory for the output video and performance stats\n",
    "2. targeted device (e.g. CPU,GPU,MYRIAD)\n",
    "3. the floating precision to use for inference\n",
    "\n",
    "The job scheduler will use the contents of `-F` flag as the argument to the job script.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 3.2 Job queue submission\n",
    "\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 5 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel Core CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel \n",
    "    Core i5-6500TE</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_core = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te -F \"results/core CPU FP32\" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if not job_id_core:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Xeon CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel \n",
    "    Xeon Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"results/xeon CPU FP32\" -N obj_det_xeon\n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if not job_id_xeon:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel Core CPU and using the onboard Intel GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:intel-hd-530 -F \"results/gpu GPU FP16\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if not job_id_gpu:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with  IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a> . The inference workload will run on the <a href=\"https://www.ieiworld.com/mustang-f100/en/\"> IEI Mustang-F100-A10 </a> card installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-f100-a10 -F \"results/fpga HETERO:FPGA,CPU FP16\" -N obj_det_fpga\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if not job_id_fpga:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel NCS 2 (Neural Compute Stick 2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://software.intel.com/en-us/neural-compute-stick\">Intel Neural Compute Stick 2</a> installed in this  node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_myriadx = !qsub classification_pneumonia_job.sh -l nodes=1:tank-870:i5-6500te:intel-ncs2 -F \"results/myriadx MYRIAD FP16 \" -N obj_det_myriadx\n",
    "print(job_id_myriadx[0]) \n",
    "#Progress indicators\n",
    "if not job_id_myriadx:\n",
    "    print(\"Error in job submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with UP Squared Grove IoT Development Kit (UP2)\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/up-squared-grove-dev-kit\">UP Squared Grove IoT Development Kit</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/96488/Intel-Atom-x7-E3950-Processor-2M-Cache-up-to-2-00-GHz-\">Intel Atom® x7-E3950 Processor</a>. The inference  workload will run on the integrated Intel® HD Graphics 505 card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Submit job to the queue\n",
    "job_id_up2 = !qsub classification_pneumonia_job.sh -l nodes=1:up-squared -F \"results/up2 GPU FP16\" -N obj_det_up2\n",
    "print(job_id_up2[0]) \n",
    "#Progress indicators\n",
    "if not job_id_up2:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. \n",
    "\n",
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs complete before proceeding to the next step.\n",
    "\n",
    "### 3.4 View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "We also saved the probability output and inference time for each input image in the folder `results/` for each architecture. \n",
    "We observe the results below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Core CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/core/result\"+job_id_core[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/xeon/result\"+job_id_xeon[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel Integrated GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/gpu/result\"+job_id_gpu[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the IEI Mustang-F100-A10 (Intel® Arria® 10 FPGA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/fpga/result\"+job_id_fpga[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the Intel NCS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/myriadx/result\"+job_id_myriadx[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Result on the UP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file=\"results/up2/result\"+job_id_up2[0]+\".txt\"\n",
    "file_ready= os.path.isfile(result_file)\n",
    "if file_ready:\n",
    "    f=open(result_file)\n",
    "    print(f.read())\n",
    "else: \n",
    "    print(\"The results are not ready yet, please retry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Assess Performance\n",
    "\n",
    "The total average time of each inference task is recorded in `results/{ARCH}/statsjob_id.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_list = [('core', 'Intel Core\\ni5-6500TE\\nCPU'),\n",
    "             ('xeon', 'Intel Xeon\\nE3-1268L v5\\nCPU'),\n",
    "             ('gpu', ' Intel Core\\ni5-6500TE\\nGPU'),\n",
    "             ('fpga', ' IEI Mustang\\nF100-A10\\nFPGA'),\n",
    "             ('myriadx', 'Intel\\nNCS2'),\n",
    "             ('up2', 'Intel Atom\\nx7-E3950\\nUP2/GPU')]\n",
    "\n",
    "stats_list = []\n",
    "for arch, a_name in arch_list:\n",
    "    if 'job_id_'+arch in vars():\n",
    "        stats_list.append(('results/'+arch+'/stats'+vars()['job_id_'+arch][0]+'.txt', a_name))\n",
    "    else:\n",
    "        stats_list.append(('placeholder'+arch, a_name))\n",
    "\n",
    "#print(stats_list)\n",
    "\n",
    "summaryPlot(stats_list, 'Architecture', 'Time, milliseconds', 'Inference Engine Processing Time', 'time' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
