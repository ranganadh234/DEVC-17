{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Application - Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This topic demonstrates how to use the Benchmark Application to estimate deep learning inference performance on supported devices. Performance can be measured for two inference modes: synchronous (latency-oriented) and asynchronous (throughput-oriented).\n",
    "\n",
    "### How it works\n",
    "If you run the application in the synchronous mode, it creates one infer request and executes the Infer method. If you run the application in the asynchronous mode, it creates as many infer requests as specified in the -nireq command-line parameter and executes the StartAsync method for each of them. If -nireq is not set, the demo will use the default value for specified device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to locate person in the frame.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### Creating IR Model\n",
    "\n",
    "The Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existing DNN models from popular frameworks (e.g. Caffe*, TF) using the Model Optimizer. \n",
    "The Intel® Distribution of OpenVINO™ toolkit includes a utility script `model_downloader.py` that you can use to download some common models. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in a command line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "Some of these downloaded models are already in the IR format, while others will require the model optimizer. In this demo, we will be using the **emotion-recognition-retail-0003** model, which is already in IR format. This model can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name emotions-recognition-retail-0003 -o models/\n",
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name emotions-recognition-retail-0003-fp16 -o models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "With the `-o` option set as above, this command downloads the model in the directory `models`, with the model files (.xml and .bin) located at `/Retail/object_attributes/emotions_recognition/0003/dldt`\n",
    "\n",
    "In the above case, the location is ~/benchmark_models/Retail/object_attributes/emotions_recognition/0003/dldt/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize OpenVINO env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "source /opt/intel/openvino/bin/setupvars.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the application with the -h option yields the following usage message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 python/benchmark_app/benchmark_app.py -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile benchmarkapp_python.sh\n",
    "set -x\n",
    "INPUT_FILE=$1\n",
    "FP_MODEL=$2\n",
    "DEVICE=$3\n",
    "API=$4\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:~/iot-devcloud/benchmark_app/python/benchmark_app\n",
    "\n",
    "if [ \"$FP_MODEL\" == \"FP16\" ]; then\n",
    "  FPEXT='-fp16'\n",
    "fi\n",
    "\n",
    "echo $INPUT_FILE\n",
    "\n",
    "! python3 python/benchmark_app/benchmark_app.py \\\n",
    "-m models/Retail/object_attributes/emotions_recognition/0003/dldt/emotions-recognition-retail-0003${FPEXT}.xml \\\n",
    "-i $INPUT_FILE \\\n",
    "-d $DEVICE \\\n",
    "-api $API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep compnode | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you want to use your own image, change the environment variable 'IMAGE' in the following cell from \"~/benchmark_app/sample_image.png\" to the full path of your uploaded image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"IMAGE\"] = os.getcwd()+\"/emotions.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Job queue submission\n",
    "\n",
    "Each of the cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all jobs at once or follow one at a time. \n",
    "\n",
    "After submission, it will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The log files with output are stored in the below location.\n",
    "~/benchmark_app/python/benchmark_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the default time for the application to run and provide output, i.e Latency and Throughput values, the output logs will be generated after 60 seconds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Xeon® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88178/Intel-Xeon-Processor-E3-1268L-v5-8M-Cache-2-40-GHz-\">Intel® \n",
    "    Xeon® Processor E3-1268L v5</a>. The inference workload will run on the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Xeon CPU FP32 - Sync...\")\n",
    "job_id_xeon = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"$IMAGE FP32 CPU async\" -N benchmark_xeon\n",
    "print(job_id_xeon[0]) \n",
    "while True:\n",
    "    var=job_id_xeon[0].split(\".\")\n",
    "    file=\"benchmark_xeon.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel Xeon CPU FP32 - Async...\")\n",
    "job_id_xeon = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:e3-1268l-v5 -F \"$IMAGE FP32 CPU async\" -N benchmark_xeon\n",
    "print(job_id_xeon[0]) \n",
    "while True:\n",
    "    var=job_id_xeon[0].split(\".\")\n",
    "    file=\"benchmark_xeon.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with Intel® Core CPU and using the onboard Intel® GPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core i5-6500TE</a>. The inference workload will run on the Intel® HD Graphics 530 card integrated with the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 bit - GPU - Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel GPU...\")\n",
    "job_id_gpu = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te -F \"$IMAGE FP16 GPU async\" -N benchmark_gpu\n",
    "print(job_id_gpu[0]) \n",
    "while True:\n",
    "    var=job_id_gpu[0].split(\".\")\n",
    "    file=\"benchmark_gpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 16 bit - GPU - Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel GPU...\")\n",
    "job_id_gpu = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te -F \"$IMAGE FP16 GPU sync\" -N benchmark_gpu\n",
    "print(job_id_gpu[0]) \n",
    "while True:\n",
    "    var=job_id_gpu[0].split(\".\")\n",
    "    file=\"benchmark_gpu.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32 bit - GPU - Async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel GPU...\")\n",
    "job_id_gpu32 = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te -F \"$IMAGE FP32 GPU async\" -N benchmark_gpu32\n",
    "print(job_id_gpu32[0]) \n",
    "while True:\n",
    "    var=job_id_gpu32[0].split(\".\")\n",
    "    file=\"benchmark_gpu32.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32 bit - GPU - Sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel GPU...\")\n",
    "job_id_gpu32 = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te -F \"$IMAGE FP32 GPU sync\" -N benchmark_gpu32\n",
    "print(job_id_gpu32[0]) \n",
    "while True:\n",
    "    var=job_id_gpu32[0].split(\".\")\n",
    "    file=\"benchmark_gpu32.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IEI Mustang-V100-MX8 ( Intel® Movidius™ Myriad™ X Vision Processing Unit (VPU))\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank 870-Q170</a> edge node with an <a href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel Core i5-6500te CPU</a>. The inference workload will run on an <a \n",
    "    href=\"https://www.ieiworld.com/mustang-v100/en/\">IEI Mustang-V100-MX8 </a>accelerator installed in this node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel VPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_hddl = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-v100-mx8 -F \"$IMAGE FP16 HDDL async\" -N benchmark_hddl\n",
    "print(job_id_hddl[0])\n",
    "while True:\n",
    "    var=job_id_hddl[0].split(\".\")\n",
    "    file=\"benchmark_hddl.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting job to Intel VPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_hddl = !qsub benchmarkapp_python.sh -l nodes=1:tank-870:i5-6500te:iei-mustang-v100-mx8 -F \"$IMAGE FP16 HDDL sync\" -N benchmark_hddl\n",
    "print(job_id_hddl[0])\n",
    "while True:\n",
    "    var=job_id_hddl[0].split(\".\")\n",
    "    file=\"benchmark_hddl.o\"+var[0]\n",
    "    if os.path.isfile(file): \n",
    "        ! cat $file\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
